{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365d408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Model 1 classes: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "Model 2 classes: {0: 'Green Light', 1: 'Left turn', 2: 'Red Light', 3: 'Traffic Light', 4: 'Yellow Light', 5: 'Bump', 6: 'No_Parking', 7: 'No_Stopping', 8: 'No_U-Turn', 9: 'Pedestrian', 10: 'Road_Work', 11: 'Speed_Limit_120', 12: 'Speed_Limit_40', 13: 'Speed_Limit_90', 14: 'Stop'}\n",
      "Video properties: 1920x1080 @ 29fps, 3899 frames\n",
      "Processing video...\n",
      "Processing frame 30/3899\n",
      "Processing frame 60/3899\n",
      "Processing frame 90/3899\n",
      "Processing frame 120/3899\n",
      "Processing frame 150/3899\n",
      "Processing frame 180/3899\n",
      "Processing frame 210/3899\n",
      "Processing frame 240/3899\n",
      "Processing frame 270/3899\n",
      "Processing frame 300/3899\n",
      "Processing frame 330/3899\n",
      "Processing frame 360/3899\n",
      "Processing frame 390/3899\n",
      "Processing frame 420/3899\n",
      "Processing frame 450/3899\n",
      "Processing frame 480/3899\n",
      "Processing frame 510/3899\n",
      "Processing frame 540/3899\n",
      "Processing frame 570/3899\n",
      "Processing frame 600/3899\n",
      "Processing frame 630/3899\n",
      "Processing frame 660/3899\n",
      "Processing frame 690/3899\n",
      "Processing frame 720/3899\n",
      "Processing frame 750/3899\n",
      "Processing frame 780/3899\n",
      "Processing frame 810/3899\n",
      "Processing frame 840/3899\n",
      "Processing frame 870/3899\n",
      "Processing frame 900/3899\n",
      "Processing frame 930/3899\n",
      "Processing frame 960/3899\n",
      "Processing frame 990/3899\n",
      "Processing frame 1020/3899\n",
      "Processing frame 1050/3899\n",
      "Processing frame 1080/3899\n",
      "Processing frame 1110/3899\n",
      "Processing frame 1140/3899\n",
      "Processing frame 1170/3899\n",
      "Processing frame 1200/3899\n",
      "Processing frame 1230/3899\n",
      "Processing frame 1260/3899\n",
      "Processing frame 1290/3899\n",
      "Processing frame 1320/3899\n",
      "Processing frame 1350/3899\n",
      "Processing frame 1380/3899\n",
      "Processing frame 1410/3899\n",
      "Processing frame 1440/3899\n",
      "Processing frame 1470/3899\n",
      "Processing frame 1500/3899\n",
      "Processing frame 1530/3899\n",
      "Processing frame 1560/3899\n",
      "Processing frame 1590/3899\n",
      "Processing frame 1620/3899\n",
      "Processing frame 1650/3899\n",
      "Processing frame 1680/3899\n",
      "Processing frame 1710/3899\n",
      "Processing frame 1740/3899\n",
      "Processing frame 1770/3899\n",
      "Processing frame 1800/3899\n",
      "Processing frame 1830/3899\n",
      "Processing frame 1860/3899\n",
      "Processing frame 1890/3899\n",
      "Processing frame 1920/3899\n",
      "Processing frame 1950/3899\n",
      "Processing frame 1980/3899\n",
      "Processing frame 2010/3899\n",
      "Processing frame 2040/3899\n",
      "Processing frame 2070/3899\n",
      "Processing frame 2100/3899\n",
      "Processing frame 2130/3899\n",
      "Processing frame 2160/3899\n",
      "Processing frame 2190/3899\n",
      "Processing frame 2220/3899\n",
      "Processing frame 2250/3899\n",
      "Processing frame 2280/3899\n",
      "Processing frame 2310/3899\n",
      "Processing frame 2340/3899\n",
      "Processing frame 2370/3899\n",
      "Processing frame 2400/3899\n",
      "Processing frame 2430/3899\n",
      "Processing frame 2460/3899\n",
      "Processing frame 2490/3899\n",
      "Processing frame 2520/3899\n",
      "Processing frame 2550/3899\n",
      "Processing frame 2580/3899\n",
      "Processing frame 2610/3899\n",
      "Processing frame 2640/3899\n",
      "Processing frame 2670/3899\n",
      "Processing frame 2700/3899\n",
      "Processing frame 2730/3899\n",
      "Processing frame 2760/3899\n",
      "Processing frame 2790/3899\n",
      "Processing frame 2820/3899\n",
      "Processing frame 2850/3899\n",
      "Processing frame 2880/3899\n",
      "Processing frame 2910/3899\n",
      "Processing frame 2940/3899\n",
      "Processing frame 2970/3899\n",
      "Processing frame 3000/3899\n",
      "Processing frame 3030/3899\n",
      "Processing frame 3060/3899\n",
      "Processing frame 3090/3899\n",
      "Processing frame 3120/3899\n",
      "Processing frame 3150/3899\n",
      "Processing frame 3180/3899\n",
      "Processing frame 3210/3899\n",
      "Processing frame 3240/3899\n",
      "Processing frame 3270/3899\n",
      "Processing frame 3300/3899\n",
      "Processing frame 3330/3899\n",
      "Processing frame 3360/3899\n",
      "Processing frame 3390/3899\n",
      "Processing frame 3420/3899\n",
      "Processing frame 3450/3899\n",
      "Processing frame 3480/3899\n",
      "Processing frame 3510/3899\n",
      "Processing frame 3540/3899\n",
      "Processing frame 3570/3899\n",
      "Processing frame 3600/3899\n",
      "Processing frame 3630/3899\n",
      "Processing frame 3660/3899\n",
      "Processing frame 3690/3899\n",
      "Processing frame 3720/3899\n",
      "Processing frame 3750/3899\n",
      "Processing frame 3780/3899\n",
      "Processing frame 3810/3899\n",
      "Processing frame 3840/3899\n",
      "Processing frame 3870/3899\n",
      "\n",
      "Processing complete!\n",
      "Output saved to: output_combined_all_again.mp4\n",
      "Total frames processed: 3899\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def process_video_with_dual_models(\n",
    "    video_path,\n",
    "    model1_path,\n",
    "    model2_path,\n",
    "    output_path=\"output_combined_all_again.mp4\",\n",
    "    conf_threshold=0.25,\n",
    "    model1_classes=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Process video with two YOLO models and draw bounding boxes from both.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input .MOV video\n",
    "        model1_path: Path to first YOLO model (.pt file)\n",
    "        model2_path: Path to second YOLO model (.pt file)\n",
    "        output_path: Path for output video\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "        model1_classes: List of class IDs to detect for model1 (None = all classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load both models\n",
    "    print(\"Loading models...\")\n",
    "    model1 = YOLO(model1_path)\n",
    "    model2 = YOLO(model2_path)\n",
    "    \n",
    "    # Get class names for both models\n",
    "    model1_classes_dict = model1.names\n",
    "    model2_classes_dict = model2.names\n",
    "    \n",
    "    print(f\"Model 1 classes: {model1_classes_dict}\")\n",
    "    print(f\"Model 2 classes: {model2_classes_dict}\")\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
    "    \n",
    "    # Define codec and create VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Different colors for different models (BGR format)\n",
    "    color_model1 = (0, 255, 0)    # Green for model 1\n",
    "    color_model2 = (255, 0, 0)    # Blue for model 2\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    print(\"Processing video...\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 30 == 0:  # Progress update every 30 frames\n",
    "            print(f\"Processing frame {frame_count}/{total_frames}\")\n",
    "        \n",
    "        # Run inference with both models\n",
    "        results1 = model1(frame, conf=conf_threshold, verbose=False)\n",
    "        results2 = model2(frame, conf=conf_threshold, verbose=False)\n",
    "        \n",
    "        # Draw detections from model 1\n",
    "        for result in results1:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                \n",
    "                # Get confidence and class\n",
    "                conf = float(box.conf[0])\n",
    "                cls = int(box.cls[0])\n",
    "                \n",
    "                # Filter by classes for model1\n",
    "                if model1_classes is not None and cls not in model1_classes:\n",
    "                    continue\n",
    "                \n",
    "                class_name = model1_classes_dict[cls]\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color_model1, 2)\n",
    "                \n",
    "                # Draw label with background\n",
    "                label = f\"{class_name}: {conf:.2f}\"\n",
    "                (label_width, label_height), _ = cv2.getTextSize(\n",
    "                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
    "                )\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (x1, y1 - label_height - 10),\n",
    "                    (x1 + label_width, y1),\n",
    "                    color_model1,\n",
    "                    -1\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    label,\n",
    "                    (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0, 0, 0),\n",
    "                    1\n",
    "                )\n",
    "        \n",
    "        # Draw detections from model 2\n",
    "        for result in results2:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                \n",
    "                # Get confidence and class\n",
    "                conf = float(box.conf[0])\n",
    "                cls = int(box.cls[0])\n",
    "                class_name = model2_classes_dict[cls]\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color_model2, 2)\n",
    "                \n",
    "                # Draw label with background\n",
    "                label = f\"{class_name}: {conf:.2f}\"\n",
    "                (label_width, label_height), _ = cv2.getTextSize(\n",
    "                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
    "                )\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (x1, y1 - label_height - 10),\n",
    "                    (x1 + label_width, y1),\n",
    "                    color_model2,\n",
    "                    -1\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    label,\n",
    "                    (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (255, 255, 255),\n",
    "                    1\n",
    "                )\n",
    "        \n",
    "        # Write the frame\n",
    "        out.write(frame)\n",
    "    \n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "    print(f\"Total frames processed: {frame_count}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    VIDEO_PATH = \"IMG_6589.MOV\"  # Change this to your video path\n",
    "    MODEL1_PATH = \"yolov8n.pt\"  # Change this to your first model path\n",
    "    MODEL2_PATH = \"traffic_lights_and_signs_real_car.pt\"\n",
    "    OUTPUT_PATH = \"video_processed.mp4\"\n",
    "    CONFIDENCE_THRESHOLD = 0.25\n",
    "    \n",
    "    # Classes to detect for model 1 (only these classes will be processed)\n",
    "    MODEL1_CLASSES = [0, 1, 2, 3, 5, 7, 9, 11]\n",
    "    \n",
    "    # Process video\n",
    "    process_video_with_dual_models(\n",
    "        video_path=VIDEO_PATH,\n",
    "        model1_path=MODEL1_PATH,\n",
    "        model2_path=MODEL2_PATH,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        conf_threshold=CONFIDENCE_THRESHOLD,\n",
    "        model1_classes=MODEL1_CLASSES\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
